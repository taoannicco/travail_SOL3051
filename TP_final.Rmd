---
title: "TP_final"
output: html_document
date: "2023-04-05"
editor_options: 
  markdown: 
    wrap: sentence
---

```{r Packages}
#install.packages("tree")
#install.packages("gbm")
library(partykit)
library(tree)
library(mice)
library(tidyverse)
library(glmnet)
library(dplyr)
library(ggplot2)
library(CUFF)
library(haven)
library(knitr)
library(xtable)
library(pairwise)
library(latex2exp)
library(ISLR2)
library(gbm)
```

## Arbres de classifications :

Utilisation des arbres de classification pour analyser l'ensemble de données de la thèse d'Éric Lacourse.

On commence par imputer les données afin de ne pas avoir de valeurs manquantes :

1.  Importation des données

```{r message=FALSE, error=FALSE}
# Importation données
#input_data <- read.csv("~/Desktop/Hiver 2023/SOL3051/TP1/Donnees_these_Lacourse.csv" , na.strings = c("#NULL!", "NA"))
#ou
input_data <- read.csv("Donnees_these_Lacourse.csv", na.strings = c("#NULL!", "NA"))
```

2.  Sélection des variables

```{r pressure, echo=FALSE}
# Sélection des variables
data_v1 <- input_data[c("age", "travp","travm", "negp9", "negm9", "dets4", "dets13", "deta4", "deta13", "mus2", 
         "muspre", "mustemp", "mushr", "alien1", "alien2", "sui1", "sui8", "drog1","drog2", "drog3", "drog4")]

```

3.  Imputation

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Imputation
summary(data_v1)
data_v1$travp[which(is.na(data_v1$travp))] = mean(data_v1$travp, na.rm = TRUE)
data_v1$travm[which(is.na(data_v1$travm))] = mean(data_v1$travm, na.rm = TRUE)
data_v1$negp9[which(is.na(data_v1$negp9))] = mean(data_v1$negp9, na.rm = TRUE)
data_v1$dets4[which(is.na(data_v1$dets4))] = mean(data_v1$dets4, na.rm = TRUE)
data_v1$dets13[which(is.na(data_v1$dets13))] = mean(data_v1$dets13, na.rm = TRUE)
data_v1$deta4[which(is.na(data_v1$deta4))] = mean(data_v1$deta4, na.rm = TRUE)
data_v1$deta13[which(is.na(data_v1$deta13))] = mean(data_v1$deta13, na.rm = TRUE)
data_v1$mus2[which(is.na(data_v1$mus2))] = mean(data_v1$mus2, na.rm = TRUE)
data_v1$muspre[which(is.na(data_v1$muspre))] = mean(data_v1$muspre, na.rm = TRUE)
data_v1$mustemp[which(is.na(data_v1$mustemp))] = mean(data_v1$mustemp, na.rm = TRUE)
data_v1$mushr[which(is.na(data_v1$mushr))] = mean(data_v1$mushr, na.rm = TRUE)
data_v1$alien1[which(is.na(data_v1$alien1))] = mean(data_v1$alien1, na.rm = TRUE)
data_v1$alien2[which(is.na(data_v1$alien2))] = mean(data_v1$alien2, na.rm = TRUE)
data_v1$sui1[which(is.na(data_v1$sui1))] = mean(data_v1$sui1, na.rm = TRUE)
data_v1$sui8[which(is.na(data_v1$sui8))] = mean(data_v1$sui8, na.rm = TRUE)
data_v1$drog1[which(is.na(data_v1$drog1))] = mean(data_v1$drog1, na.rm = TRUE)
data_v1$drog2[which(is.na(data_v1$drog2))] = mean(data_v1$drog2, na.rm = TRUE)
data_v1$drog3[which(is.na(data_v1$drog3))] = mean(data_v1$drog3, na.rm = TRUE)
data_v1$drog4[which(is.na(data_v1$drog4))] = mean(data_v1$drog4, na.rm = TRUE)
summary(data_v1)
```

On recode la variable `sui8` en variable binaire qu'on nomme `tentative_suicide`.

```{r, warning=FALSE, message=FALSE}
# sui1 et sui 8 
# Code binaire 1 = oui ; 0 = non
attach(data_v1)
tentative_suicide <- factor(ifelse(sui8 <= 1, "Oui", "Non"))
data_v1 <- data.frame(data_v1, tentative_suicide)
```

On utilise maintenant la fonction `tree()` pour créer un arbre de classification afin de prédire `tentative_suicide` en utilisant toutes les variables sauf `sui8`.
La syntaxe de la fonction `tree()` est assez similaire à celle de la fonction `lm()`.

```{r chunk4}
tree.data.v1 <- tree(tentative_suicide ~ . - sui8, data_v1)
```

La fonction `summary()` liste les variables utilisées comme noeuds internes de l'arbre, le nombre de noeuds terminaux et le taux d'erreur (d'entrainement).

```{r chunk5}
summary(tree.data.v1)
```

On observe que le taux d'erreur d'entrainement est de 6%.

On utilise la fonction `plot()` pour afficher la structure de l'arbre, et la fonction `text()` pour afficher les étiquettes des noeuds.

L'argument `pretty = 0` indique à `R` d'inclure les noms de catégories pour tous les prédicteurs qualitatifs, plutôt que de simplement afficher une lettre pour chaque catégorie.

```{r polt1}
plot(tree.data.v1)
text(tree.data.v1, pretty = 0)
```

L'indicateur le plus important de `sui1` semble être l'emplacement des rayonnages, puisque la première branche différencie les emplacements inférieurs à la valeur 1.28 et ceux supérieurs à 1.28.


```{r chunk7}
tree.data.v1
```

Afin d'évaluer correctement la performance d'un arbre de classification sur ces données, on doit estimer l'erreur de test plutôt que de simplement calculer l'erreur d'apprentissage.

On divise donc les observations en un ensemble d'entraînement et un ensemble de test, on construit l'arbre en utilisant l'ensemble d'entraînement, et on évalue sa performance sur les données de test.
On utilise pour cela la fonction `predict()`.

```{r chunk8}
set.seed(2)
train <- sample(1:nrow(data_v1), 200)
data_v1.test <- data_v1[-train, ]
suicide.test <- tentative_suicide[-train]
tree.data.v1 <- tree(tentative_suicide ~ . - sui8, data_v1,
                      subset = train)
tree.pred <- predict(tree.data.v1, data_v1.test,
                     type = "class")
table(tree.pred, suicide.test)

(84 + 4) / 200
```

Dans le cas d'un arbre de classification, l'argument `type = "class"` indique à `R` de renvoyer la prédiction de classe réelle.

Cette approche permet d'obtenir des prédictions correctes pour environ 44% des emplacements de l'ensemble de données de test.

(Si on exécute à nouveau la fonction `predict()`, on peut obtenir des résultats légèrement différents, en raison "d'égalités": par exemple, cela peut se produire lorsque les observations d'apprentissage correspondant à un noeud terminal sont réparties de manière égale entre les valeurs de réponse `Oui` et `Non`).

Ensuite, on examine si l'élagage de l'arbre peut permettre d'améliorer les résultats.

La fonction `cv.tree()` effectue une validation croisée afin  ; l'élagage par la complexité des coûts (*cost complexity pruning*) est utilisé afin de sélectionner une séquence d'arbres à prendre en compte.

On utilise l'argument `FUN = prune.misclass` pour indiquer qu'on veut que le taux d'erreur de classification guide le processus de validation croisée et d'élagage, plutôt que la valeur par défaut de la fonction `cv.tree()`, qui est la déviance.
La fonction `cv.tree()` indique le nombre de noeuds terminaux de chaque arbre considéré (`size`) ainsi que le taux d'erreur correspondant et la valeur du paramètre de coût-complexité utilisé (`k`).

```{r chunk9}
set.seed(7)
cv.df <- cv.tree(tree.data.v1, FUN = prune.misclass)
names(cv.df)
cv.df
```

Il y a un seul groupe pour un degré de compléxité de 6 noeuds dans la validation croisée

```{r chunk10}
par(mfrow = c(1, 2))
plot(cv.df$size, cv.df$dev, type = "b", xlab = "Nombre de noeuds", ylab = "Erreurs de validation croisée")
plot(cv.df$k, cv.df$dev, type = "b", xlab = "Degrés de compléxité" ,ylab = "Groupes")
```

On applique maintenant la fonction `prune.misclass()` afin d'élaguer l'arbre pour obtenir l'arbre à 9 noeuds.

```{r chunk11}
prune.df <- prune.misclass(tree.data.v1, best = 9)
plot(prune.df)
text(prune.df, pretty = 0)
```


```{r chunk12}
tree.prediction <- predict(prune.df, data_v1.test,
    type = "class")
table(tree.prediction, suicide.test)

(89 + 4) / 200
```

Maintenant, 46.5% des observations de test sont correctement classées, donc non seulement le processus d'élagage a produit un arbre plus interprétable, mais il a aussi légèrement amélioré la précision de la classification.

Si on augmente la valeur de `best`, on obtiens un arbre élagué plus grand avec une précision de classification plus faible :

```{r chunk13}
prune.df <- prune.misclass(tree.data.v1, best = 12)
plot(prune.df)
text(prune.df, pretty = 0)
tree.prediction <- predict(prune.df, data_v1.test,
    type = "class")
table(tree.prediction, suicide.test)
(84 + 4) / 200
```

## Arbres de régression :

```{r chunk14}
set.seed(1)
train <- sample(1:nrow(data_v1), nrow(data_v1) / 2)

tree.data.v1 <- tree(tentative_suicide ~ . -sui8, data_v1, subset = train)

summary(tree.data.v1)
```

Dans le contexte d'un arbre de régression, la déviance est simplement la somme des erreurs au carré pour l'arbre.
On visualise maintenant l'arbre : 

```{r chunk15}
plot(tree.data.v1)
text(tree.data.v1, pretty = 0)
```

On utilise maintenant la fonction `cv.tree()` pour voir si l'élagage de l'arbre améliore sa performance : 

```{r chunk16}
cv.data.v1 <- cv.tree(tree.data.v1)
plot(cv.data.v1$size, cv.data.v1$dev, type = "b", xlab = "Nombre de noeuds", ylab = "Complexité")
```

Dans ce cas, l'arbre le plus complexe considéré est sélectionné par validation croisée.
Cependant, si on veut élaguer l'arbre, on peut le faire en utilisant la fonction `prune.tree()` :

```{r chunk17}
prune.data.v1 <- prune.tree(tree.data.v1, best = 7)
plot(prune.data.v1)
text(prune.data.v1, pretty = 0)
```

Les prunning a des résultats similaires aux précédents arbres. 

Conformément aux résultats de la validation croisée, on utilise l'arbre non élagué pour faire des prédictions sur l'ensemble de test.

```{r chunk18}
yhat <- predict(tree.data.v1, newdata = data_v1[-train,])

data.v1.test <- data_v1[-train, "sui8"]

yhat <- sample(yhat, size = 152, replace = FALSE)

plot(yhat, data.v1.test)
abline(0, 1)
sprintf("L'erreur résiduelle (MSE) de l'ensemble de test associée à l'arbre de régression est de %.1f%%", mean((yhat - data.v1.test)^2))
```

## Bagging et forêts aléatoires

On applique la procédure de *bagging* et les forêts aléatoires aux données de `Data_v1`, en utilisant la bibliothèque `randomForest` dans `R`.

Les résultats exacts obtenus dans cette section peuvent dépendre de la version de `R` et de la version du paquet `randomForest` installée sur votre ordinateur.

Rappelez-vous que le *bagging* est simplement un cas particulier de forêt aléatoire avec `m=p`.
Par conséquent, la fonction `randomForest()` peut être utilisée pour effectuer à la fois les forêts aléatoires et le *bagging*.
On commence par faire le *bagging*

Pour les étapes préliminaires, nous avions besoin de la variable tentative_suicide afin d'avoir une variable codée Oui ou Non selon les résultats pour sui8.
Pour le reste de l'étude, nous n'avons plus besoin de cette dernière et nous la soustrairons aux modèles observés.

```{r chunk19}
#install.packages("randomForest")
library(randomForest)
```

```{r chunk19/2}
set.seed(1)
bag.data.v1 <- randomForest(sui8 ~ . -tentative_suicide, data = data_v1,
    subset = train, mtry = 20, importance = TRUE)

bag.data.v1

```

L'argument `mtry = 20` indique que les 20 prédicteurs doivent tous être pris en compte pour chaque division de l'arbre - en d'autres termes, que le *bagging* doit être effectuée.

Quelle est la performance de ce modèle avec *bagging* sur l'ensemble de test ?

La MSE de l'ensemble de test associé à l'arbre de régression avec *bagging* est de 23,42, soit environ deux tiers de ce qu'on avait obtenu à l'aide d'un arbre unique à élagage optimal.
On peut modifier le nombre d'arbres cultivés par `randomForest()` en utilisant l'argument `ntree` :

```{r chunk21}
bag.data.v1 <- randomForest(sui8 ~ .-tentative_suicide, data = data_v1,
    subset = train, mtry = 20, ntree = 25)
yhat.bag <- predict(bag.data.v1, newdata = data_v1[-train, ])

mean((yhat.bag - data.v1.test)^2)
```

L'utilisation d'une forêt aléatoire se déroule exactement de la même manière, sauf qu'on une valeur plus petite de l'argument `mtry`.
Par défaut, `randomForest()` utilise $p/3$ variables lors de la construction d'une forêt aléatoire d'arbres de régression, et $\sqrt{p}$ variables lors de la construction d'une forêt aléatoire d'arbres de classification.
Ici, on utilise `mtry = 6`.

```{r chunk22}
set.seed(1)
rf.data.v1 <- randomForest(sui8 ~ .-tentative_suicide, data = data_v1,
    subset = train, mtry = 20, importance = TRUE)
yhat.rf <- predict(rf.data.v1, newdata = data_v1[-train, ])
mean((yhat.rf - data.v1.test)^2)
```

La MSE de l'ensemble de test est de 20,07 ce qui indique que les forêts aléatoires ont permis une amélioration par rapport au *bagging* dans ce cas.

En utilisant la fonction `importance()`, on peut voir l'importance de chaque variable.

```{r chunk23}
importance(rf.data.v1)
```

Deux mesures de l'importance des variables sont présentées.
La première est basée sur la diminution moyenne de la précision des prédictions sur les échantillons hors sac lorsqu'une variable donnée est permutée.
La seconde est une mesure de la diminution totale de l'impureté des noeuds qui résulte des divisions sur cette variable, en moyenne sur tous les arbres.
Dans le cas des arbres de régression, l'impureté des noeuds est mesurée par le RSS de formation, et pour les arbres de classification par la déviance.
On peut produire des graphiques de ces mesures d'importance à l'aide de la fonction `varImpPlot()`.

```{r chunk24}
varImpPlot(rf.data.v1)
```

Les résultats indiquent que pour tous les arbres considérés dans la forêt aléatoire, la richesse de la communauté (`lstat`) et la taille de la maison (`rm`) sont de loin les deux variables les plus importantes.

## Boosting

Pour le *boosting*, on utilise le paquet `gbm` et la fonction `gbm()` pour créer des arbres de régression boostés à l'ensemble de données `Boston`.
On exécute `gbm()` avec l'option `distribution = "gaussian"` puisqu'il s'agit d'un problème de régression ; s'il s'agissait d'un problème de classification binaire, on utiliserait plutôt `distribution = "bernoulli"`.
L'argument `n.trees = 5000` indique qu'on veut 5000 arbres, et l'option `interaction.depth = 4` limite la profondeur de chaque arbre.

Étant donné que nous reconcentrons l'étude sur la variable sui8, la distribution ne sera pas de bernouilli mais plutôt gaussienne.

```{r chunk25}
library(gbm)
```

```{r chunk25/2}
set.seed(1)
boost.data.v1 <- gbm(sui8 ~ .-tentative_suicide, data = data_v1[train, ],
    distribution = "gaussian", n.trees = 5000,
    interaction.depth = 4)
```

La fonction `summary()` produit un graphique d'influence relative et sort également les statistiques d'influence relative.

```{r chunk26}
summary(boost.data.v1)
```

Encore une fois on voit que `alien1` et `drog1` sont de loin les variables les plus importantes.
On peut aussi produire des graphiques de dépendance partielle pour ces deux variables.
Ces graphiques illustrent l'effet marginal des variables sélectionnées sur la réponse après avoir intégré les autres variables.
Dans ce cas, comme nous pouvions nous y attendre, les prix médians des maisons augmentent avec `rm` et diminuent avec `lstat`.

```{r chunk27}
plot(boost.data.v1 , i = "alien1")
plot(boost.data.v1, i = "drog1")
```

On utilise maintenant le modèle boosté pour prédire `SUI8` sur les données de test:

```{r chunk28}
yhat.boost <- predict(boost.data.v1,
    newdata = data_v1[-train, ], n.trees = 5000)
mean((yhat.boost - data.v1.test)^2)
```

La MSE de test obtenue est de 18.39 : c'est supérieur à l'erreur de test des forêts aléatoires et du bagging.
Si on veut, on peut effectuer le boosting avec une valeur différente pour le paramètre de rétrécissement $\lambda$.
La valeur par défaut est de 0.001, mais elle est facilement modifiable.
Ici, on utilise $\lambda=0.2$.

```{r chunk29}
boost.data.v1 <- gbm(sui8 ~ .-tentative_suicide, data = data_v1[train, ],
    distribution = "gaussian", n.trees = 5000,
    interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.data.v1,
    newdata = data_v1[-train, ], n.trees = 5000)
mean((yhat.boost - data.v1.test)^2)
```

Dans ce cas-ci, utiliser $\lambda=0.2$ donne une plus petite erreur de test que $\lambda=0.001$.

```{r BART}
#install.packages("BART")
library(BART)
```

```{r BART/2}
x <- data_v1[, 1:21]
y <- data_v1[, "sui8"]
xtrain <- x[train,]
ytrain <- y[train]
xtest <- x[-train,]
ytest <- x[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
```
