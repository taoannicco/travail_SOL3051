---
output:
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: xelatex
title: "ISLR laboratoire chapitre 8: Arbres de décisions"
toc: true
---


Ce laboratoire est adapté du laboratoire proposé dans le chapitre 8 du livre *Introduction to Statistical Learning* de Gareth James, Daniela Witten, Trevor Hastie et Robert Tibshirani.

*DeepL* a été utilisé pour la traduction.


##Arbres de classification

La bibliothèque `tree` est utilisée pour construire des arbres de classification et de régression.

```{r chunk1}
library(tree)
```

On utilise d'abord les arbres de classification pour analyser l'ensemble de données `Carseats`. Dans cette base de données, `Sales` est une variable continue, donc on commence par la recoder en  variable binaire. On utilise la fonction `ifelse()` pour créer une variable, appelée `High`, qui prend la valeur `Yes` si la variable `Sales` dépasse 8 et prend la valeur `Non` sinon.

```{r chunk2}
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
```

Enfin, on utilise la fonction `data.frame()` pour fusionner `High` avec le reste des données `Carseats`.

```{r chunk3}
Carseats <- data.frame(Carseats, High)
```

We now  use the `tree()` function to fit a classification tree in order to predict `High` using all variables but `Sales`.
The syntax of the `tree()` function is quite similar to that of the `lm()` function.

On utilise maintenant la fonction `tree()` pour créer un arbre de classification afin de prédire `High` en utilisant toutes les variables sauf `Sales`. La syntaxe de la fonction `tree()` est assez similaire à celle de la fonction `lm()`.

```{r chunk4}
tree.carseats <- tree(High ~ . - Sales, Carseats)
```

La fonction `summary()` liste les variables utilisées comme noeuds internes de l'arbre, le nombre de noeuds terminaux et le taux d'erreur (d'entrainement).

```{r chunk5}
summary(tree.carseats)
```

On voit que le taux d'erreur d'entrainement est de 9%.

Pour les arbres de classification, la déviance rapportée dans la sortie de `summary()` est donnée par
\[
-2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk},
\]
où $n_{mk}$ est le nombre d'observations du $m$ème noeud terminal qui appartiennent à la $k$ème classe. Une faible déviance indique un arbre qui offre une bonne adéquation aux données (d'entraînement). La *déviance moyenne résiduelle* rapportée est simplement la déviance divisée par $n-|{T}_0|$, qui dans ce cas est de 400-27=373.

L'une des propriétés les plus attrayantes des arbres est qu'ils peuvent être représentés graphiquement. On utilise la fonction `plot()` pour afficher la structure de l'arbre, et la fonction `text()` pour afficher les étiquettes des noeuds. L'argument `pretty = 0` indique à `R` d'inclure les noms de catégories pour tous les prédicteurs qualitatifs, plutôt que de simplement afficher une lettre pour chaque catégorie.

```{r chunk6}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

L'indicateur le plus important de `Sales` semble être l'emplacement des rayonnages, puisque la première branche différencie les emplacements `Bon` des emplacements `Mauvais` et `Moyen`.

Si on tape simplement le nom de l'objet arbre, `R` imprime la sortie correspondant à chaque branche de l'arbre. `R` affiche le critère de division (par exemple, `Prix < 92.5`), le nombre d'observations dans cette branche, la déviance, la prédiction globale pour la branche (`Oui` ou `Non`) et la fraction d'observations dans cette branche qui prennent les valeurs `Oui` et `Non`. Les branches qui représentent des noeuds terminaux sont indiquées par des astérisques.

```{r chunk7}
tree.carseats
```

Afin d'évaluer correctement la performance d'un arbre de classification sur ces données, on doit estimer l'erreur de test plutôt que de simplement calculer l'erreur d'apprentissage. On divise donc les observations en un ensemble d'entraînement et un ensemble de test, on construit l'arbre en utilisant l'ensemble d'entraînement, et on évalue sa performance sur les données de test. La fonction `predict()` peut être utilisée à cette fin.

Dans le cas d'un arbre de classification, l'argument `type = "class"` indique à `R` de renvoyer la prédiction de classe réelle. Cette approche permet d'obtenir des prédictions correctes pour environ 77% des emplacements de l'ensemble de données de test.

```{r chunk8}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats,
    subset = train)
tree.pred <- predict(tree.carseats, Carseats.test,
    type = "class")
table(tree.pred, High.test)
(104 + 50) / 200
```

(Si on exécute à nouveau la fonction `predict()`, on peut obtenir des résultats légèrement différents, en raison "d'égalités": par exemple, cela peut se produire lorsque les observations d'apprentissage correspondant à un noeud terminal sont réparties de manière égale entre les valeurs de réponse `Oui` et `Non`).

Ensuite, on examine si l'élagage de l'arbre peut permettre d'améliorer les résultats. La fonction `cv.tree()` effectue une validation croisée afin de déterminer le niveau optimal de complexité de l'arbre ; l'élagage par la complexité des coûts (*cost complexity pruning*) est utilisé afin de sélectionner une séquence d'arbres à prendre en compte.

On utilise l'argument `FUN = prune.misclass` pour indiquer qu'on veut que le taux d'erreur de classification guide le processus de validation croisée et d'élagage, plutôt que la valeur par défaut de la fonction `cv.tree()`, qui est la déviance. La fonction `cv.tree()` indique le nombre de noeuds terminaux de chaque arbre considéré (`size`) ainsi que le taux d'erreur correspondant et la valeur du paramètre de coût-complexité utilisé (`k`).

```{r chunk9}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```

MAlgré son nom, `dev` corresponds aux nombre d'erreur de validation croisée
cross-validation errors. L'arbre avec 9 noeuds terminaux ne donne lieu qu'à 74 erreurs de validation croisée.  On visualise ensuite le taux d'erreur en fonction de `size` et de`k`.

```{r chunk10}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

On applique maintenant la fonction `prune.misclass()` afin d'élaguer l'arbre pour obtenir l'arbre à neuf noeuds.

```{r chunk11}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

Quelle est la performance de cet arbre élagué sur l'ensemble de données de test ? Une fois de plus, on utilise la fonction `predict()`.

```{r chunk12}
tree.pred <- predict(prune.carseats, Carseats.test,
    type = "class")
table(tree.pred, High.test)
(97 + 58) / 200
```

Maintenant, 77.5% des observations de test sont correctement classées, donc non seulement le processus d'élagage a produit un arbre plus interprétable, mais il a aussi légèrement amélioré la précision de la classification.

Si on augmente la valeur de `best`, on obtiens un arbre élagué plus grand avec une précision de classification plus faible :
```{r chunk13}
prune.carseats <- prune.misclass(tree.carseats, best = 14)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test,
    type = "class")
table(tree.pred, High.test)
(102 + 52) / 200
```

## Arbres de régression

Dans cette partie, on crée un arbre de régression sur l'ensemble de données `Boston`. Tout d'abord, on crée un ensemble d'entraînement et on construit l'arbre sur les données d'entraînement.

```{r chunk14}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

Remarquez que la sortie de `summary()` indique que seules quatre des variables ont été utilisées pour construire l'arbre. Dans le contexte d'un arbre de régression, la déviance est simplement la somme des erreurs au carré pour l'arbre. On visualise maintenant l'arbre.

```{r chunk15}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

La variable `lstat` mesure le pourcentage de personnes ayant un statut socio-économique faible, tandis que la variable `rm` correspond au nombre moyen de pièces. L'arbre indique que des valeurs plus grandes de `rm`, ou des valeurs plus faibles de `lstat` correspondent à des maisons plus chères. Par exemple, l'arbre prédit un prix médian de 45 400$ pour les maisons situées dans les secteurs de recensement dans lesquels `rm >= 7,553`.

Il est intéressant de noter qu'on aurait pu créer un arbre beaucoup plus grand, en utlissant l'argument `control = tree.control(nobs = length(train), mindev = 0)` dans la fonction `tree()`.

On utilise maintenant la fonction `cv.tree()` pour voir si l'élagage de l'arbre améliore sa performance.

```{r chunk16}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

Dans ce cas, l'arbre le plus complexe considéré est sélectionné par validation croisée. Cependant, si on veut élaguer l'arbre, on peut le faire en utilisant la fonction `prune.tree()` :

```{r chunk17}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```
Conformément aux résultats de la validation croisée, on utilise l'arbre non élagué pour faire des prédictions sur l'ensemble de test.

```{r chunk18}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
```
Autrement dit, l'erreur résiduelle (MSE) de l'ensemble de test associée à l'arbre de régression est de 35,29. La racine carrée de  MSE est donc d'environ 5,94, ce qui indique que ce modèle conduit à des prédictions de test qui sont (en moyenne) à 5 941$ près de la vraie valeur médiane des maisons pour le secteur de recensement.

## Bagging et forêts aléatoires

On applique la procédure de *bagging* et les forêts aléatoires aux données de `Boston`, en utilisant la bibliothèque `randomForest` dans `R`. Les résultats exacts obtenus dans cette section peuvent dépendre de la version de `R` et de la version du paquet `randomForest` installée sur votre ordinateur.

Rappelez-vous que le *bagging* est simplement un cas particulier de forêt aléatoire avec
`m=p`. Par conséquent, la fonction `randomForest()` peut être utilisée pour effectuer à la fois les forêts aléatoires et le *bagging*. On commence par faire le *bagging*

```{r chunk19}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 12, importance = TRUE)
bag.boston
```

L'argument `mtry = 12` indique que les 12 prédicteurs doivent tous être pris en compte pour chaque division de l'arbre - en d'autres termes, que le *bagging* doit être effectuée.
Quelle est la performance de ce modèle avec *bagging* sur l'ensemble de test ?

```{r chunk20}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```
La MSE de l'ensemble de test associé à l'arbre de régression avec *bagging* est de 23,42, soit environ deux tiers de ce qu'on avait obtenu à l'aide d'un arbre unique à élagage optimal.
On peut modifier le nombre d'arbres cultivés par `randomForest()` en utilisant l'argument `ntree` :

```{r chunk21}
bag.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```

L'utilisation d'une forêt aléatoire se déroule exactement de la même manière, sauf qu'on une valeur plus petite de l'argument `mtry`. Par défaut, `randomForest()` utilise $p/3$ variables lors de la construction d'une forêt aléatoire d'arbres de régression, et $\sqrt{p}$ variables lors de la construction d'une forêt aléatoire d'arbres de classification. Ici, on utilise `mtry = 6`.

```{r chunk22}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

La MSE de l'ensemble de test est de 20,07 ce qui  indique que les forêts aléatoires ont permis une amélioration par rapport au *bagging* dans ce cas.

En utilisant la fonction `importance()`, on peut voir l'importance de chaque variable.

```{r chunk23}
importance(rf.boston)
```

Deux mesures de l'importance des variables sont présentées. La première est basée sur la diminution moyenne de la précision des prédictions sur les échantillons hors sac lorsqu'une variable donnée est permutée. La seconde est une mesure de la diminution totale de l'impureté des noeuds qui résulte des divisions sur cette variable, en moyenne sur tous les arbres. Dans le cas des arbres de régression, l'impureté des noeuds est mesurée par le RSS de formation, et pour les arbres de classification par la déviance. On peut produire des graphiques de ces mesures d'importance à l'aide de la fonction `varImpPlot()`.

```{r chunk24}
varImpPlot(rf.boston)
```
Les résultats indiquent que pour tous les arbres considérés dans la forêt aléatoire, la richesse de la communauté (`lstat`) et la taille de la maison (`rm`) sont de loin les deux variables les plus importantes.

## Boosting

Pour le *boosting*, on utilise le paquet `gbm` et la fonction `gbm()` pour créer des arbres de régression boostés à l'ensemble de données `Boston`. On exécute `gbm()` avec l'option `distribution = "gaussian"` puisqu'il s'agit d'un problème de régression ; s'il s'agissait d'un problème de classification binaire, on utiliserait plutôt `distribution = "bernoulli"`.
L'argument `n.trees = 5000` indique qu'on veut 5000 arbres, et l'option `interaction.depth = 4` limite la profondeur de chaque arbre.

```{r chunk25}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
    distribution = "gaussian", n.trees = 5000,
    interaction.depth = 4)
```

La fonction `summary()` produit un graphique d'influence relative et sort également les statistiques d'influence relative.

```{r chunk26}
summary(boost.boston)
```

Encore une fois on voit que `lstat` et `rm` sont de loin les variables les plus importantes. On peut aussi produire des graphiques de dépendance partielle pour ces deux variables. Ces graphiques illustrent l'effet marginal des variables sélectionnées sur la réponse après avoir intégré les autres variables. Dans ce cas, comme nous pouvions nous y attendre, les prix médians des maisons augmentent avec `rm` et diminuent avec `lstat`.

```{r chunk27}
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```

On utilise maintenant le modèle boosté pour prédire `medv` sur les données de test:

```{r chunk28}
yhat.boost <- predict(boost.boston,
    newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

La MSE de test obtenue est de 18.39 : c'est supérieur à l'erreur de test des forêts aléatoires et du bagging. Si on veut, on peut effectuer le boosting avec une valeur différente pour le paramètre de rétrécissement $\lambda$. La valeur par défaut est de 0.001, mais elle est facilement modifiable.
Ici, on utilise $\lambda=0.2$.

```{r chunk29}
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
    distribution = "gaussian", n.trees = 5000,
    interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston,
    newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

Dans ce cas-ci, utiliser $\lambda=0.2$ donne une plus petite erreur de test que $\lambda=0.001$.


